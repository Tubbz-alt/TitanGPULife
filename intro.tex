% !TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{section:introduction}

The Cray XK7 Titan supercomputer~\cite{olcf:titan} was the \#1 system in the world for a
very long time~\cite{top500list}, and has remained a critically important computer
system through the end of its life in the Summer of 2019. It defied
scale with 18,688 individual compute nodes and delivered tens of
billions of computing hours to the U.S. Department of Energy
mission-critical programs for nearly 7 years.
 
From a realiability perspective, Titan was a very interesting
machine. Its operation was forced to execute three very significant
rework cycles, two on the mechanical assembly affecting the PCIe
connector from the GPU daughtercard to the motherboard, and a
replacement of about 11,000 GPU assemblies because of a failing
resistor on a printed circuit board. We write primarily about the epoch
that includes this last rework cycle. This includes Titan's most
stable and failure free period and contains the most reliable data on
GPU operation.

Figure~\ref{fig:chronology} illustrates the chronology of the rework
cycles and stable periods as indicated by the number of GPU changes at
periodic inventories.
\begin{figure*}[tbp]
  \centering
  \includegraphics[width=\textwidth]{figs/chronology001.pdf}
  \caption{Volume of GPU swaps detected at inventories. The break-in
    period before 2014 involved working out mechanical assembly issues
    at PCIe connectors
    frequent dates.}
  \label{fig:chronology}
\end{figure*}
The first two rework cycles before 2014 can be characterized as a
break-in period on a new system that is first of its kind and pushes
many technological boundaries, in this case mainly regarding blade
mechanical assembly. The resulting field engineering generated high
GPU swap counts. Late 2013 begins a very long period of stable
operation with very few issues until about mid 2016, when GPU failures
begin to rise. This results in the final rework cycle, replacing
11,000 GPUs throughout much of 2017.

The failures were traced to a failing resistor on the circuitboard
(not the GPU chip iself). Throughout this paper, when we refer to
replacing the GPU, we refer to the entire circuitboard along with its
GPU chip. While the 2016 resistor failures were a surprise, there is a
known phenomenon~\cite{referenceneeded} that describes a two phase
process that leads to failure. It also explains why there was a very
stable period preceding the failures. There is specific decay
associated with the electronic components where, as these products
age, they will eventually fail thresholds for guard band margin and
other things. In a lot of cases, they will just fail as
well. \fix{Give a reference and expand?}

Our analysis focuses on GPUs that were installed in the second rework
cycle and later, essentialy units installed near the beginning of 2014
and later. We will make the data as well as our analysis codes
producing the graphics available in a public repository (see
\fix{reference?}).  The data used in this study, while only a 7 MB
file, represents over 100,000 years of operation and may be the
largest publicly available GPU reliability data set.

We begin by describing related work in Sec.~\ref{section:related}. Our
data collection is desctibed in Sec.~\ref{section:dataprep} and the
data cleaning, checking, and lifetime summarization process in
Sec.~\ref{section:dataclean}. A time between failures (TBF) analysis
is given in Sec.~\ref{section:tbf}. Survival analysis based on
Kaplan-Meier modeling (KM) and Cox regression modeling (CR) of
relative hazard rates in various locations are in
Sec.~\ref{section:survival}. Finally, we discuss our main conclusions
and recommendations in Sec.~\ref{section:discussion}.
 


