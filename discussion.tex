% !TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\rev{Conclusions}}
\label{section:discussion}

\rev{The failure rates of the old batch are not matched by the new
  batch nor by experience at other facilities with the same type of
  GPU components, making this an unexpected event that had
  considerable impact on operations and on availability of the
  system. On a positive but ironic note, the corrosion process in the
  failing resistors made the specific GPU batch act as sensitive
  instruments for obtaining cummulative trend information from
  component heat dynamics, giving the current analysis the strong
  signals observed, and provided lessons learned. Here we dicsuss the
  conclusions we can draw from the experience on mitigation, which
  keeps the system operating at an acceptable level and possibly even
  restores some of the lost capability, and on long-term planning,
  which addresses such scenarios in future-generation supercomputers.}

%%%%%%%%%%%%
% Mitigation
%%%%%%%%%%%%
\subsection{Mitigation}
\label{section:mitigation}

In Titan, the mitigation component included replacing about 11,000 GPUs and
changing the job scheduling strategy~\cite{8665764}. Replacing about 59\% of
Titan's 18,688 GPUs helped to improve productivity by restoring some of the lost
capability, but it was a costly and time-consuming effort. Changing the job
scheduling strategy to run larger jobs on more reliable and smaller jobs on less
reliable nodes played a crucial role in maintaining productivity at reduced
capability. However, jobs running on larger portions of the system, utilizing
most or all of its resources, were still impacted by the failing GPUs and
corresponding low system MTBF.

The options of replacing failed components and employing reliability-aware
resource management may not be available or be cost effective for other
supercomputing centers dealing with similar unexpected reliability issues.
%
Replacement components may not be readily available and may have to be
manufactured, which can be impossible for a technology that is no longer supported
by the original manufacturer. Experienced errors or failures may be caused by
software, which can be difficult to fix if the software itself or the deployed
version is no longer supported by the vendor.

There is also a financial aspect, such as a service contract and/or warranty
expiration and the operational budget of a supercomputing center not having
significant funds for replacement/reengineering costs or a manufacturer or
vendor not paying for them.
%
There is additionally the time and financial cost for root-cause analysis and
for developing a hardware and/or software mitigation strategy in the first place.
Fixing a problem requires finding and understanding it, which can be difficult 
in today's complex systems and may require knowledge that only the original 
manufacturer or vendor possesses.

Reliability-aware resource scheduling also has its limitations, as the network
architecture needs to be taken into account. Titan's 3D torus network creates
certainly more challenges for efficient job allocations, than the fat tree
network of Titan's successor, Summit~\cite{olcf:summit}. Nodes associated with
the same job need to be close to each other on the network for maximum
application performance. Also, node outages or less reliable nodes within a
tight group of reliable nodes create resource allocation
holes. Separating two lower nodes in each cage 0 on Titan would not
create a contiguous partition, yet this group would be the most
reliable in 2016 as was shown in the preceeding section.
%
Alternatively, partially or completely replacing the aging and failing
supercomputer with a new system, even if this solution offers less capability,
may be more cost effective in the end.

Other mitigation components for Titan included matching the checkpointing
interval of applications with the system's current
reliability~\cite{bautista-gomez16reducing, 6903564}. The experienced issue here
was the lack of automated communication of system reliability information and
the lack of flexibility in application-level checkpoint/restart implementations.
Constantly measuring and reporting current system reliability is not an automated
real-time task in many currently deployed supercomputers. Applications are
typically not designed to skip a checkpoint when system reliability is good or
take more frequent checkpoints when system reliability is getting worse.

%%%%%%%%%%%%%%%%%%%%
% Long-term Planning
%%%%%%%%%%%%%%%%%%%%
\subsection{Long-term Planning}
\label{section:planning}

The long-term planning component is one of the biggest lessons learned from the
Titan reliability experience. Today's supercomputers are designed to deal with
expected reliability issues. However, history has shown~\cite{geist12kill} that
unexpected reliability issues do occur and do have a significant impact. Vendors
and manufacturers obviously can not mitigate against all possible reliability
threats, however, a resilience strategy is needed for future-generation systems
that is able to deal with emerging unexpected reliability threats in a
reasonable and cost effective way. In addition to mitigation, better support for
automated real-time reliability monitoring and reporting is needed, including
for root-cause analysis.

Supercomputer center policies regarding reliability monitoring, resource
allocation and checkpointing strategies need to be powerful and flexible enough
to facilitate mitigation for such unexpected reliability threats. System
acquisition contracts may need to include performance requirements for degraded
operation, such as a certain percentage of performance capability if the MTBF
drops by an order of magnitude.

\rev{The experienced reliability issues with Titan had a direct impact on the
development and deployment of Titan's successor, Summit, and even on Summit's
successor, Frontier. As a result, more and better monitoring data is already
collected on Summit. NVIDIA's GPU management and monitoring software has been
significantly improved. Temperature monitoring has been significantly improved
as well.}