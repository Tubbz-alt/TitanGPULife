% !TEX root = main.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Related Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\label{section:related}

Being the largest machine with the most GPUs in the world attracts a lot of attention so particularly during Titan's first few years of operation, many studies were published about different aspects of its reliability. However, none of the studies investigate the atypical failure mode discussed in this paper nor do they consider data over the full lifetime of the system through its decomissioning.

Tiwari et al~\cite{Tiwari15Experience,7056044} analyze types of GPU errors on Titan and on a GPU cluster at Los Alamos National Laboratory. This also includes neutron beam experiments at the Los Alamos Neutron Science Center and at Rutherford Appleton Laboratories to further understand GPU soft errors caused by neutron radiation triggered upsets (bit flips and timing errors).
%
Tiwari et al~\cite{10.1145/2807591.2807666} further analyze types of GPU errors on Titan, including software/firmware related errors and failures, ECC double-bit errors, and GPU ``off the bus'' and ECC page retirement events. This work observes an ECC double-bit mean-time between errors (MTBE) of about 160 hours, or about one per week, with 86\% occurring in GPU memory and the rest in the GPU register file. The GPU ``off the bus'' events were caused by a system integration issue that was fixed and not an inherent GPU or GPU memory architecture flaw.
%
Nie et al.~\cite{7446091,nie17characterizing,nie18machine} characterize the soft error behavior of Titan's GPUs in relation to temperature and power consumption to predict their increased occurrence by correlating data in temporal and spatial domains and using machine learning models. The work primarily focuses on correctable single-bit errors.
%
Tiwari et al. and Nie et al. do not address the atypical failure mode discussed in this paper. They also only use Titan error and failure data from June 2013 to February 2015, excluding early failures and the failure mode discussed in this paper (which occurred after February 2015).

Zimmer et al.~\cite{8665764} develop a new job scheduling strategy for Titan to counter the GPU failures that we discuss and to improve system utilization and productivity. The solution uses reordering of the compute nodes for resource allocation, scheduling larger jobs on more reliable and smaller jobs on less reliable nodes.

Ezell~\cite{osti_1086655} creates a general understanding of Titan's interconnect failures using an application to stress test Titan's Gemini interconnect.
%
The work by Kumar et al.~\cite{kumar18understanding} analyzes Titan's interconnect faults, errors and congestion events to improve resilience of the interconnects and congestion resolution methods. The results show that the magnitude of interconnect errors is very high with an uneven distribution across different types of links. They also show that congestion is very frequent and bursty.
%
Gupta et al.~\cite{7266836} investigate the spatial and temporal properties of failures on Titan and their impact on resilience mechanisms and the implications for efficient system operation.
%
Gupta et al.~\cite{gupta17failures} also perform a study covering five supercomputers at Oak Ridge National Laboratory, including Titan, that concentrates on developing an understanding of which and how many errors and failures occur in supercomputers over multiple generations and over their years of operation. This work resulted in a lot of lessons learned, including that the mean-time between failures (MTBF) can change drastically and non-monotonically over time.
%
Meneses et al.~\cite{Meneses15Analyzing} analyze the interplay and workload on Titan using failure and job submission logs. The results indicate that failures depend heavily on workload.

Bautista-Gomez et al.~\cite{bautista-gomez16reducing} use failure data from Titan to adapt the checkpoint frequency to the current reliability of the system using dynamic checkpointing.
%
Tiwari et al.~\cite{6903564} also use failure data from Titan and exploit temporal locality in the data to adapt the checkpoint frequency to the current reliability of the system using lazy checkpointing.
%
Both approaches, Bautista-Gomez et al. and Tiwari et al., aim at dealing with the drastically and non-monotonically changing MTBF of Titan to match current system reliability with an efficient recovery strategy.

Other work in characterizing supercomputer faults, errors and failures focuses on the various other systems deployed around the world, primarily at US Department of Energy laboratories.
%
Di et al.~\cite{8809553} develop an in-depth understanding of failure characteristics of the IBM Blue Gene/Q Mira system at Argonne National Laboratory. This work shows that 99.4\% of job failures are due to user behavior, such as errors in the code or misconfiguration. Martino et al.~\cite{6903615} characterize the errors and failures of the Blue Waters Cray XE6/XK7 system at the National Center for Supercomputing Applications, at the University of Illinois at Urbana-Champaign. The results show that 74.4\% of system-wide outages in Blue Waters were caused by software. Blue Waters's XK7 partition had the same architecture as Titan, but did not experience the same GPU failure mode detailed in this paper.
% Interesting ... do we know why?