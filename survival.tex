\section{Survival Analysis}
\label{sec:survival}
\fix{George's survival analysis, check \protect\pkg{finalfit}} 


Kaplan-Meier survival analysis gives the probability of survival beyond a given time. It is a nonparametric technique that makes no specific failure model assumptions, such as Weibull, Exponential, etc. It can also split the data into subpopulations and compute separate survival curves.

If $T$ is taken to be the random variable of a GPU fail time, then its probability density function is $f(t)$ and its cumulative distribution function $F(t) = Pr\{T < t\}$ gives the probability that a GPU fails by duration $t$. The survival function is then
\begin{displaymath}
  S(t) = Pr\{T \geq t\} = 1 - F(t) = \int_t^\infty f(x)dx,
\end{displaymath}
which is the probability of being operational just before duration $t$.

A sister concept to the survival function is the hazard function:
\begin{displaymath}
  H(t) = \frac{f(t)}{S(t)},
\end{displaymath}
which is what is sometimes called the "bathtub" curve in reliability.


The Cox proportional hazards (CPH) regression analys \cite{Cox1972,Harrell2015} takes the hazard function above and a set of covariates $x$ and models $k$s GPU hazard as
\begin{displaymath}
  H_k(t) = H_0(t)e^{\sum\limits_{i=1}^n{\beta x}}
\end{displaymath}
That is, a base hazard rate multiplied by a function of
covariates. The key assumption in the CPH model is that the "hazards"
are multipliers on the "baseline hazard" but the estimate of the
baseline hazard is nonparametric, meaning it makes no specific
distributional assumption and just learns from the data (unlike
analyses that assume for example a Weibul or an exponential
model). The multiplicative assumption is that the hazard curves do not
cross and are multiples of each other. The multiplier is the *hazard
coefficient*. For example, if the baseline is *node0* and the *node2*
hazard is 2, then *node2* sees twice as many failures as *node0* on
average. 

For our comparison of hazards, we use as *covariates* all the unpacked
location variables, indicators whether the GPU has been moved, and our
notion of old and new GPUs based on first install date. Since many
GPUs experience time at more than one location, we use the longest
period location in this analysis. 

First, we do a univariate analysis for each covariate separately and
then we look at a multivariate view of this. Having multiple
covariates is called "multiple regression" as opposed to "multivariate
regression" reserved for cases with several response variables. The
univariate view is comparable to what would one see in a simple time
between failures analysis for a given population split. It can be
affected by sampling differences for other covariates. For example, if
the strongest effect is seen in a variable not used in a split and its
values are unevenly distributed across the split, its effect can leak
into the split. This is why we also do a multivariate analysis, where
the effect of each covariate is adjusted for all the other
covariates. 
 

\begin{figure}
  \includegraphics[width=5in]{cox.pdf}
  \caption{GPU hazard ratios from Cox regression model.}
\end{figure}

Higher numbered columns appear to have a higher (up to about 2x)
failure rate. There actually seems to be a hint of three groups that
have similar rates: 0-6 baseline, 7-16 are 1.5x baseline, and 17-24
are 2x baseline. 

Higher numbered rows tend to have a lower (up to about 0.5x) failure rate.

This seems surprisingly strong! Higher numbered cages have failure
rates 5x and 16x higher than cage = 0. In fact, this seems so strong
that its effect could be leaking into all other comparisons under
uneven sampling!

Not much difference between slots. They are all close to the baseline
slot0.

Interestingly, nodes 2 and 3 have half the failure rate of nodes 0 and
1. Is this related to the cage variable or is there a real location
effect?

Both loc and move are roughly the same, indicating that GPUs that were
moved to another location had slighly (.75x) lower failure rate. This
may just reflect a “repair effect” that extends the life of a GPU. It
may have been removed and reinstalled for a reason that was not
recorded. 
 
Oddly, the older batch has a very slighly lower hazard, although there
is a lot of uncertainty here, indicating that something more
complicated is at work and needs deeper investigation. 

Now the multivariate Cox regression analysis, where each factor is
adjusted for all the other factors. This can fix some uneven sampling
and multivariate dependence. 

The col groups persist and are even stronger. The row effect is also
stronger. The cage effect is out of this world! slot is still without
any effect. node effect also persists and is a bit stronger. 

It is strange that move and loc effects are opposite, whereas in the
univariate analysis they were the same. They are also much
stronger. The batch effect is opposite of what it was under univariate
analysys and a bit stronger as well. This is a concern and it may be
related to violation of the “proportionality assumption” of hazards
not crossing. We see some crossing in the survival curves above
already. 

Let’s separate the “old” and “new” batches of GPUs and run separate
Cox models. 





\begin{figure}
  \includegraphics[width=5in]{glmnet.pdf}
  \caption{GPU variable importance from glmnet penalized regression analysis.}
\end{figure}
