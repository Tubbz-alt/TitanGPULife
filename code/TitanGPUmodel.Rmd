---
title: "Titan GPU models"
author: "George Ostrouchov"
output:
  html_document:
    css: style.css
    theme: cerulean
---

R scripts to accompany SC20 paper "GPU Lifetimes on Titan Supercomputer: Survival Analysis and Reliability" by George Ostrouchov, Don Maxwell, Rizwan Ashraf, Christian Engelmann, Mallikarjun Shankar, and James Rogers.


```{r setup}
options(repos = c(CRAN = "http://cran.rstudio.com")) # set CRAN mirror
installed = installed.packages()
if(! "remotes" %in% installed) install.packages("remotes")
if(! ("forestmodel" %in% installed && packageVersion("forestmodel") >= "0.6.1"))
  remotes::install_github("NikNakk/forestmodel") # only package from GitHub
## all other packages will prompt to install from CRAN
library(data.table)
library(magrittr)
library(dplyr)
library(ggplot2)
library(forcats)
library(lubridate)
library(survival)
library(survminer)
library(forestmodel)
sessionInfo()
```

The above documents software versions and platform that was used to run the analysis.
```{r}
out_figs = "../figs/" # paper related figures will go here
data_dir = "../data/" # input files live here
history_file = paste0(data_dir, "titan.gpu.history.txt")
service_nodes = paste0(data_dir, "titan.service.txt")

source("TitanGPUsetup.R") ## define some additional functions
Rmd.time = proc.time()
```

**Read the data**. 

Last inventory files processing was done on 01/20/2020 but Titan was turned off on 08/01/2019 so change these *remove* dates.
```{r}
g_raw = fread(history_file, col.names = c("SN", "location", "insert", "remove"))
lastrun_dates = stringr::str_starts(g_raw$remove, "01/20/2020")
g_raw$remove[lastrun_dates] = "08/01/2019 20:07:33"
cat("Total of", sum(lastrun_dates), "remove dates changed to lights-out date")
cat("Total of", (unique_SN = length(unique(g_raw$SN))), "unique SN\n")
cat("Total of", (unique_loc = length(unique(g_raw$location))),
    "unique locations\n")
```

**Enable DST corrected dates** for *insert* and *remove* with Eastern time zone.
**Enable missing values**, and **create** some new variables.
Variable *event* is "life" when both *insert* and *remove* are present, "life0" when *insert* = *remove*, otherwise it is whatever string is in *insert*, which is "DBE" or "Off The BUS" (re-coded to "OTB"). 
**Fill in** (repeat) serial numbers and locations for all records.
```{r}
time_zone = "America/New_York"
g_ev = g_raw %>% 
  mutate(event = if_else(is.na(lubridate::mdy_hms(insert, quiet = TRUE)),
                         insert,  # if not a date, event is the insert string
                         "life"), # if a date, it's a life span
         event = if_else(event == "Off The BUS", "OTB", event), # recode "Off The BUS" to "OTB"
         location = na_if(location, ""),  # blank to NA for zoo later
         insert = lubridate::mdy_hms(insert, tz = time_zone, quiet = TRUE),
         remove = lubridate::mdy_hms(remove, tz = time_zone), # !quiet incase NA
         duration = lubridate::as.duration(remove - insert), # in seconds
         event = as.factor(if_else(event == "life" & as.numeric(duration) == 0,
                                   "life0", event)),
         SN = ifelse(SN == "", as.character(NA), SN), # blank to NA for zoo
         SN = zoo::na.locf(SN), # fill in missing SN (forward)
         SN_ok = grepl("[0-9]{13}", SN), # verify that SN is 13 numbers
         location = zoo::na.locf(location), # fill in missing location forward
         location_ok = grepl("c[0-9][0-9]?-[0-7]c[0-2]s[0-7]n[0-3]", location))
```

**List** bad SN and bad location raw data. **Remove** bad SN and bad location data. Cause is unknown but there are very few.
```{r}
good_date_remove = lubridate::mdy_hms("01/01/2014 00:00:00", tz = time_zone)
good_date_insert = lubridate::mdy_hms("11/01/2013-00:00:00", tz = time_zone)
good_date_last = lubridate::mdy_hms("12/01/2016 00:00:00", tz = time_zone)

g_raw[!g_ev$SN_ok, ]  # list bad SN records
g_raw[!g_ev$location_ok, ] # list bad location records
```

**Read** service nodes and remove associated data.
```{r}
service = fread(service_nodes, col.names = c("node", "loc", "location", "type", "up", "run"))
cat("Total of", (unique_sloc = length(unique(service$location))),
    "unique service locations\n")
service_notseen =
  service$location[!(service$location %in% unique(g_ev$location))]
cat("Service locations not referenced in data:", service_notseen, "\n")

gc_ev = g_ev %>%
  filter(SN_ok, location_ok, !(location %in% service$location)) %>%
  select(-SN_ok, -location_ok) # remove service node data (must be after zoo!)
cat("Total of", (unique_SN = length(unique(gc_ev$SN))), "unique clean SN\n")
cat("Total of", (unique_loc = length(unique(gc_ev$location))),
    "unique clean locations\n")
```

Make a chronology plot of Titan GPU life. Overlay yearly histogram over daily histogram. Yearly excludes data before *good_date_remove*. Both plots are restricted to "life" records only. (Note to reviewers: an earlier manuscript version included "life0" "DBE" "OTB" events in this chronology, artificially inflating GPU swap numbers, mostly in the first year.)
```{r  fig.width = 10}
ggp(ggplot(filter(g_ev %>% filter(event == "life"), remove < max(remove)), aes(remove)) + 
      geom_histogram(
        data = filter(g_ev, remove >= good_date_remove, remove < max(remove)),
        mapping = aes(remove), binwidth = 60*60*24*365, alpha = 0.2,
        boundary = good_date_remove) +
      geom_histogram(binwidth = 60*60*24, color = "blue") + theme_bw() +
      theme(axis.title.x = element_blank()) + scale_x_datetime(breaks = "1 year"), #+
#      scale_y_continuous(limits = c(0, 9250), breaks = seq(0, 9000, 1000)),
    file = "chronology", width = 10, height = 3)
```

Begin data selection by removing all GPU life times that have *remove* date before *good_date* and all that have zero life times. The premise is that record keeping was different and possibly inconsistent before *r good_date*. **gc_ev_nzg** is used in many analyses going forward. Five original SNs remain after this and are removed manually to have a clean "post 2nd rework cycle" data set. 
```{r}
gc_ev_nzg = gc_ev %>%
  filter(remove > good_date_remove, # drop GPUs removed before good_date!
         !(SN %in% c("0323712022786", "0323712007923", "0323712007994",
                   "0323712022956", "0323712042970")), # rm insert dates < 2013-06
         event != "life0") %>% mutate(event = fct_drop(event))  # remove life0's
last_inventory = max(gc_ev_nzg$remove, na.rm = TRUE) # last right-censoring date
cat("gc_ev dim", dim(gc_ev), "processed into gc_ev_nzg dim", dim(gc_ev_nzg), "\n")
cat("before good_date", sum(gc_ev$remove <= good_date_remove, na.rm = TRUE), ": life0",
    sum(gc_ev$event == "life0", na.rm = TRUE), "remove is NA", sum(is.na(gc_ev$remove)), "\n")
```

**Mark time overlaps within SN, then remove the overlap record and plot before-after**. 
SN time overlaps are possible when an inventory is incomplete. A unit is not detected as removed, yet is detected as inserted in a different location - thus the SN appears at both locations until the next inventory.
```{r fig.height = 10}
gc_life = mark.overlaps(gc_ev_nzg %>% filter(event == "life") %>%
                          select(-event), SN)
gc_ev_fail = gc_ev_nzg %>% select(SN, remove, location, event) %>%
  filter(event != "life") %>% arrange(SN, remove, location) %>%
  mutate(event = fct_drop(event))
plot.life_ev(gc_life, gc_ev_nzg, SN, overlaps = TRUE, outs = FALSE, file = "overlap_SN")
```

Now the same SN after removing overlap records:
```{r fig.height=10}
gc_life = gc_life %>% filter(!overlap_rec)
plot.life_ev(gc_life, gc_ev_nzg, SN, overlaps = TRUE, outs = FALSE, file = "overlap_SN_after") 
```

For locations, **remove only the overlapping life** but keep the rest of the SN. The overlaps seem very short so this makes sense. Why do location overlaps exist? In some cases, they seem to be the full blade of 4 GPUs. Blades were sometimes hot-swapped by CRAY and if this was during an inventory run, the results could be unpredictable.
```{r fig.height = 10}
gc_life = mark.overlaps(gc_life, location)
plot.life_ev(gc_life, gc_ev_nzg, location, overlaps = TRUE, outs = FALSE, 
             file = "overlap_Loc")
```

Print raw data for a few overlap locations to understand reasons:
```{r}
for(i in which(g_raw$location == "c7-7c0s7n3")) print(g_raw[(i-5):(i+5), ])
for(i in which(g_raw$location == "c22-0c2s0n0")) print(g_raw[(i-5):(i+5), ])
```

**Plot overlap at locations again after removing the offending life spans.**
```{r fig.height = 10}
gc_life = gc_life %>% filter(!overlap_rec)
plot.life_ev(gc_life, gc_ev_nzg, location, overlaps = TRUE, outs = FALSE, 
             file = "overlap_Loc_after")
```
After removing overlaps, determine *out* = "last seen" record that may indicate censoring
```{r}
gc_life = gc_life %>% group_by(SN) %>%
  mutate(out = if_else(remove == max(remove) & remove < last_inventory,
                                     TRUE, FALSE)) %>% ungroup()
```

**Inventory interval analysis**. Use differences between sorted unique *insert* and *remove* dates of "life" records to determine inventory intervals.
```{r fig.width=10, fig.height=4}
life_insert = gc_life$insert # collect insert dates
life_remove = gc_life$remove # collect remove dates
dates_life = sort(unique(c(life_insert, life_remove))) # unique insert,remove

life_intervals =
  data.frame(days = as.numeric(diff(dates_life))/(60*60*24),
             year = factor(as.character(year(dates_life[-1])))) %>% 
  filter(days >= 1) %>% 
  mutate(days = factor(as.integer(floor(days)), levels = 1:56))
ggp(ggplot(life_intervals, aes(days, fill = year)) + stat_count() +
      scale_x_discrete(breaks = seq(1, 56, 2), drop = FALSE) + theme_bw(),
    file = "attention_intervals", height = 3)
```

Seems longest interval without an inventory is 56 days!

Next, sample a few SN and locations to display a sense of GPU life for the paper.
```{r}
## set sampling for SN and Locations display
nsample = 90 # the max number for a full page pair of figures
set.seed(76513)  # for reproducibility!
fig_height = as.integer(nsample*0.1 + 0.5)
```

**Plot a sample** of `r nsample` GPUs to check that things look right ...
* starts: black dots (gray for zero lifetime)
* lifetimes: black lines
* DBEs: red triangles
* OTB: blue triangles
* Last seen: black ]
```{r}
SN_sample = sample(unique(gc_ev_nzg$SN), nsample)
ev_sample = gc_ev_nzg %>% filter(SN %in% SN_sample)
life_sample = gc_life %>% filter(SN %in% SN_sample)
plot.life_ev(life_sample, ev_sample, SN, "sample_SN", overlaps = FALSE)
```

**Plot a sample** of `r nsample` locations ...
```{r}
loc_sample = sample(unique(gc_ev_nzg$location), nsample)
ev_sample = gc_ev_nzg %>% filter(location %in% loc_sample)
life_sample = gc_life %>% filter(location %in% loc_sample) %>% 
  tidyr::separate(location, c(NA, "col", "row", "cage", "slot", "node"),
           sep = "[-csn]", convert = TRUE, remove = FALSE) %>%
  mutate(location = reorder(location, col)) # orders in increasing col
plot.life_ev(life_sample, ev_sample, location, "sample_loc", overlaps = FALSE)
```

Reduce to only "life" records and mark them with ending DBE, OTB, "out", or none events. The result is just *insert* and *remove* with *event* marks to indicate DBE or OTB or "out" (last seen). *bad* indicates that a DBE or OTB occurred yet the GPU was not taken out. For a first cut, keep the *bad* ones in. Also keep *duration* to later aggregate into full life times.

```{r}
gc_life = gc_life %>% select(SN, location, insert, remove, duration, out) %>%
  arrange(SN, remove, location)
gc_fulljoin = gc_life %>% full_join(gc_ev_fail, by = c("SN", "remove", "location"))
gc_full = gc_fulljoin %>% group_by(SN, remove, location) %>%
  mutate(insert = if_else(row_number() > 1, as.POSIXct(NA), insert),
         duration = if_else(row_number() > 1, as.numeric(NA), as.numeric(duration)),
         out = if_else(row_number() > 1, as.logical(NA), out)) %>% ungroup()
```

**Output** gc_full data frame to a CSV file for TBF analysis with Python code
```{r}
gc_write = as.data.frame(lapply(gc_full, as.character), stringsAsFactors = FALSE)
readr::write_csv(gc_write, paste0(data_dir, "gc_full.csv"), na = "", col_names = TRUE)
```

**Message to reader of file:** All fields are character strings. Dates are with timezone. Missing values as "<NA>" strings.

**Print raw records** for Figure 3:
```{r}
for(i in which(g_raw$SN == "0323812007945")) print(g_raw[(i-5):(i+5), ])
for(i in which(g_raw$SN == "0325216047736")) print(g_raw[(i-5):(i+5), ])
for(i in which(g_raw$SN == "0323812008856")) print(g_raw[(i-5):(i+5), ])
```

Next, aggregate into one record per *SN* with total life time and first *insert* time. Indicate if event occurred or still in service: out = fail, dbe = fail, otb = fail, NA = right censored (still in service). Add some other variables, such as GPU in single location or moved one or more times, mark *new_batch* group, etc. to use in modeling later.

**This is where life times are determined**, connected with a location and with an event (OTB, DBE, out). Note that location is imprecise if more than one is used (we use longest). Get proportion of time at reference location *time_max_loc*.
```{r}
gc_summary = gc_full %>% arrange(SN, remove) %>% group_by(SN) %>%
  summarize(
    time = sum(duration, na.rm = TRUE), # total life time
    nlife = sum(!is.na(duration)), # number of life records
    nloc = length(unique(location)), # Number of locations where lived
    last = max(remove), # last time stamp
    max_loc = ifelse(!all(is.na(insert)), location[which.max(duration)], ""),
    max_loc_events = ifelse(all(is.na(event)), 0,
                            length(event[location == max_loc])),
    time_max_loc = sum(duration[location == max_loc], na.rm = TRUE)/time,
    dbe = sum(event == "DBE", na.rm = TRUE),
    dbe_loc = ifelse(all(is.na(duration)) | dbe == 0, NA,
                     max_loc %in% location[event == "DBE"]),
    otb = sum(event == "OTB", na.rm = TRUE),
    otb_loc = ifelse(all(is.na(duration)) | otb == 0, NA,
                     max_loc %in% location[event == "OTB"]),
    out = any(out, na.rm = TRUE), # removed
    batch = if_else(min(insert, na.rm = TRUE) >
                      lubridate::mdy_hms("01/01/2016 00:00:00", tz = time_zone), "new", "old")
  )
gc_summary
```

**Separate max_loc** into components (col, row, cage, slot, node) to be used as covariates in survival analysis. Note that this location is the place this *SN* was located the longest. This muddies the survival analysis somewhat as several locations can be the "treatment" for a given *SN*. But the proportions of time at max_loc tend to be very high (see later analysis below). 
```{r}
gc_summary_loc = gc_summary %>% tidyr::separate(max_loc, c(NA, "col", "row", "cage", "slot", "node"),
           sep = "[-csn]", convert = TRUE, remove = TRUE) %>% # c{col}-{row}c{cage}s{slot}n{node}
  mutate( col = as.factor(col), row = as.factor(row), cage = as.factor(cage), 
          slot = as.factor(slot), node = as.factor(node),
          days = time/(60*60*24),
          years = days/365,
          dead = out & dbe + otb > 0,
          dead_otb = out & otb > 0,
          dead_dbe = out & dbe > 0) # define fail & censored

gc_summary_loc_o = gc_summary_loc %>% filter(batch == "old")
gc_summary_loc_n = gc_summary_loc %>% filter(batch == "new")
gc_summary_loc_censor1 = gc_summary_loc %>% 
  mutate(dead = if_else(batch == "old" & years > 1, FALSE, dead), 
         days = if_else(batch == "old" & years > 1, 1*365, days),
         years = if_else(batch =="old" & years > 1, 1, years))  # censor at 1 year
gc_summary_loc_censor2 = gc_summary_loc %>% 
  mutate(dead = if_else(batch == "old" & years > 2, FALSE, dead), 
         days = if_else(batch == "old" & years > 2, 2*365, days),
         years = if_else(batch =="old" & years > 2, 2, years))  # censor at 2 years
gc_summary_loc_censor3 = gc_summary_loc %>% 
  mutate(dead = if_else(batch == "old" & years > 3, FALSE, dead), 
         days = if_else(batch == "old" & years > 3, 3*365, days),
         years = if_else(batch =="old" & years > 3, 3, years))  # censor at 3 years
gc_summary_loc_censor4 = gc_summary_loc %>% 
  mutate(dead = if_else(batch == "old" & years > 4, FALSE, dead), 
         days = if_else(batch == "old" & years > 4, 4*365, days),
         years = if_else(batch =="old" & years > 4, 4, years))  # censor at 4 years
```

This is the second data set we make available.
**Output** gc_full to a CSV file
```{r}
gc_write = as.data.frame(lapply(gc_summary_loc, as.character), stringsAsFactors = FALSE)
readr::write_csv(gc_write, paste0(data_dir, "gc_summary_loc.csv"), na = "", col_names = TRUE)
```

Get some stats for sanity check.
```{r}
cat("Total", nrow(gc_summary_loc), "unique lifetimes\n")
cat("  Censored:", sum(!gc_summary_loc$dead), "  dead:", sum(gc_summary_loc$dead),
    "  Known alive:", sum(!gc_summary_loc$out), "\n")
cat("  DBE or OTB during life:", sum(gc_summary_loc$dbe > 0 | gc_summary_loc$otb >0), "\n")
cat("           More than one:", sum(gc_summary_loc$dbe + gc_summary_loc$otb > 1, na.rm = TRUE), "\n")
```

Nothing really remarkable in the stats. Censored + dead = total unique. Most with a DBE or OTB are marked dead. A few, 101 units (= 5190 - 5089) with DBEs or OTBs are still alive (that's just under 2%). 551 units (about 10%) had more than one DBE or OTB.

Look at life proportion at longest location:
```{r}
ggplot(gc_summary_loc, aes(x = time_max_loc)) + geom_histogram(bins = 200) +
  ggtitle("Life Proportion at Longest Location") + theme_bw()
ggplot((gc_summary_loc %>% filter(dead)), aes(x = time_max_loc)) +
      geom_histogram(bins = 200) + theme_bw() +
      ggtitle("Life Proportion at Longest Location - Dead Only")
```

The overwhelming majority of time is spent in the "longest" location so it is reasonable to assume the the longest location is the "treatment" of a GPU. But a time-dependent analysis is possible from the current data with some additional re-coding and following this time-dependent covariates technique: https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf. We hope that others will undertake this more detailed analysis from our data.

**SN categories**
We tried disassembling the *SN* into digits but did not find compelling relationship with life times except the known new-batch old-batch differences.

**Kaplan-Meier survival analysis** gives the probability of survival beyond a given time. It is a nonparametric technique that makes no specific failure model assumptions, such as Weibull, Exponential, etc. It can also split the data into subpopulations based on the covariates and compute separate survival curves.
```{r fig.height = 8}
ggp(ggsurvplot(survfit(
  Surv(years, dead, type = 'right') ~ node, data = gc_summary_loc),
  facet.by = c("batch", "cage"), conf.int = TRUE, risk.table = TRUE,
  ggtheme = theme_bw(), xlab = "Years", legend.title = "Node", censor.size = 2,
  legend = c(0.08, 0.73), legend.labs = 0:3, ylim = c(0.08, 1), size = 0.4, 
  break.y.by = 0.1, censor = FALSE),
  width = 10.5, height = 6, file = "km_cage-node_a")
```

The Cox proportional hazards (CPH) regression analys takes the hazard function above and a set of covariates $x$ and models $k$s GPU hazard as
$$H_k(t) = H_0(t)e^{\sum\limits_{i=1}^n{\beta x}}$$
That is, a base hazard rate multiplied by a function of covariates. The key assumption in the CPH model is that the "hazards" are multipliers on the "baseline hazard" but the estimate of the baseline hazard is nonparametric, meaning it makes no specific distributional assumption and just learns from the data (unlike analyses that assume for example a Weibul or an exponential model). The multiplicative assumption is that the hazard curves do not cross and are multiples of each other. The multiplier is the *hazard coefficient*. For example, if the baseline is *node0* and the *node2* hazard is 2, then *node2* sees twice as many failures as *node0* on average.

First, consider some diagnostics for proportionalty:
```{r fig.width = 10, fig.height = 11}
fit = coxph(Surv(years, dead, type = 'right') ~ row + col + cage + slot + node,
            data = gc_summary_loc_o)
test.ph = cox.zph(fit)
ggcoxzph(test.ph)
```

The fit lines are supposed to be horizontal for the proportionality assumption to be satisfied. They are all nearly horizontal. On the other hand, all the significance tests can detect a departure from horizontal because we have so much data. We interpret this that we have enough data to do time-dependent analysis (see comment on this above) but that the hazard proportionality is fairly good. To err on the conservative side, we use the results to make qualitative conclusions and are careful to not over-interpret the actual hazard levels.

So now the CPH model:
```{r fig.width = 5, fig.height = 10}
response = "Surv(time, event = dead) ~"
gc_summary_loc_o = gc_summary_loc %>% filter(batch == "old")
gc_summary_loc_n = gc_summary_loc %>% filter(batch == "new")

col2torus = order(c(0, seq(1, 23, 2), seq(24, 2, -2)))
panels = list(
  forest_panel(width = 0.03),
  forest_panel(width = 0.10, display = variable, fontface = "bold",
               heading = "Variable"),
  forest_panel(width = 0.05, display = level, fontface = "bold"),
  forest_panel(width = 0.05, display = n, heading = "N"),
  forest_panel(width = 0.05, display = n_events, heading = "Events"),
  forest_panel(width = 0.03, item = "vline", hjust = 0.5),
  forest_panel(width = 0.91, item = "forest", line_x = 0, hjust = 0.5,
               heading = "Hazard ratio", linetype = "dashed"),
  forest_panel(width = 0.03))

covariates = c("col", "row", "cage", "slot", "node")
levels(gc_summary_loc_o$col) = 
  paste0(formatC(levels(gc_summary_loc_o$col), width = 2),
         "   (X-", formatC(col2torus, width = 2), ")")
mv_model = coxph(as.formula(paste(response, paste(covariates, collapse = " + "))),
                 data = gc_summary_loc_o)
ggp(forest_model(mv_model, panels, factor_separate_line = TRUE), width = 5,
    height = 7, file = "cox_o")

tcol = col2torus[gc_summary_loc_o$col]
gc_summary_loc_o = gc_summary_loc_o %>% mutate(col = reorder(col, tcol))
mv_model = coxph(as.formula(paste(response, paste(covariates, collapse = " + "))),
                 data = gc_summary_loc_o)
ggp(forest_model(mv_model, panels, factor_separate_line = TRUE), width = 5,
    height = 7, file = "cox_o_t")

levels(gc_summary_loc_n$col) = 
  paste0(formatC(levels(gc_summary_loc_n$col), width = 2),
         "   (X-", formatC(col2torus, width = 2), ")")
mv_model = coxph(as.formula(paste(response, paste(covariates, collapse = " + "))),
                 data = gc_summary_loc_n)
ggp(forest_model(mv_model, panels, factor_separate_line = TRUE), width = 5,
    height = 7, file = "cox_n")

tcol = col2torus[gc_summary_loc_n$col]
gc_summary_loc_n = gc_summary_loc_n %>% mutate(col = reorder(col, tcol))
mv_model = coxph(as.formula(paste(response, paste(covariates, collapse = " + "))),
                 data = gc_summary_loc_n)
ggp(forest_model(mv_model, panels, factor_separate_line = TRUE), width = 5,
    height = 7, file = "cox_n_t")
```


```{r}
cat("Total time:\n"); print(proc.time() - Rmd.time)
```