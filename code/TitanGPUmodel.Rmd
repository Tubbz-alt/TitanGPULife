---
title: "Titan GPU model"
author: "GO"
output:
  html_document:
    css: style.css
    theme: cerulean
---
```{r setup, include=FALSE}
out_dir = "../figs/" # paper related figures will go here
in_dir = "../data/" # input files live here
history_file = paste0(in_dir, "titan.gpu.history.txt")
service_nodes = paste0(in_dir, "titan.service.txt")
source("TitanGPUsetup.R")
Rmd.time = proc.time()
```
`r today()`
Read the data. Last inventory was run 01/20/2020 but Titan was turned off on 08/01/2019
```{r}
g_raw = fread(history_file, col.names = c("SN", "location", "insert", "remove"))
g_raw$remove[stringr::str_starts(g_raw$remove, "01/20/2020")] =
  "08/01/2019 20:07:33"
cat("Total of", (unique_SN = length(unique(g_raw$SN))), "unique SN\n")
cat("Total of", (unique_loc = length(unique(g_raw$location))),
    "unique locations\n")
```

**Enable DST corrected dates** for *insert* and *remove* with Eastern time zone.
**Enable missing values**, and **create** some new variables.
Variable *event* is "life" when both *insert* and *remove* are present, "life0" when *insert* = *remove*, otherwise it is whatever string is in *insert*, which is "DBE" or "Off The BUS" (renamed to "OTB"). 
**Fill in** (repeat) serial numbers and locations for all records.
```{r}
time_zone = "America/New_York"
g_ev = g_raw %>% 
  mutate(event = if_else(is.na(lubridate::mdy_hms(insert, quiet = TRUE)),
                         insert,  # if not a date, event is the insert string
                         "life"), # if a date, it's a life span
         event = if_else(event == "Off The BUS", "OTB", event), # recode "Off The BUS" to "OTB"
         location = na_if(location, ""),  # blank to NA for zoo later
         insert = lubridate::mdy_hms(insert, tz = time_zone, quiet = TRUE),
         remove = lubridate::mdy_hms(remove, tz = time_zone), # !quiet incase NA
         duration = lubridate::as.duration(remove - insert), # in seconds
         event = as.factor(if_else(event == "life" & as.numeric(duration) == 0,
                                   "life0", event)),
         SN = ifelse(SN == "", as.character(NA), SN), # blank to NA for zoo
         SN = zoo::na.locf(SN), # fill in missing SN (forward)
         SN_ok = grepl("[0-9]{13}", SN), # verify that SN is 13 numbers
         location = zoo::na.locf(location), # fill in missing location (forward)
         location_ok = grepl("c[0-9][0-9]?-[0-7]c[0-2]s[0-7]n[0-3]", location))
```

**List** bad SN and bad location raw data. **Remove** bad SN and bad location data. **Read and remove** service node data.
```{r fig.width = 10}
good_date_remove = lubridate::mdy_hms("01/01/2014 00:00:00", tz = time_zone)
good_date_insert = lubridate::mdy_hms("11/01/2013-00:00:00", tz = time_zone)
good_date_last = lubridate::mdy_hms("12/01/2016 00:00:00", tz = time_zone)

g_raw[!g_ev$SN_ok, ]  # list bad SN records
g_raw[!g_ev$location_ok, ] # list bad location records
service = fread(service_nodes, col.names = c("node", "loc", "location", "type", "up", "run"))
cat("Total of", (unique_sloc = length(unique(service$location))),
    "unique service locations\n")
service_notseen =
  service$location[!(service$location %in% unique(g_ev$location))]
cat("Service locations not referenced in data:", service_notseen, "\n")

gc_ev = g_ev %>%
  filter(SN_ok, location_ok, !(location %in% service$location)) %>%
  select(-SN_ok, -location_ok) # remove service node data (must be after zoo!)
cat("Total of", (unique_SN = length(unique(gc_ev$SN))), "unique clean SN\n")
cat("Total of", (unique_loc = length(unique(gc_ev$location))),
    "unique clean locations\n")
top_remove_dates = sort(table(g_ev$remove), decreasing = TRUE)[1:25]
trdat = top_remove_dates[order(names(top_remove_dates))]
top_insert_dates = sort(table(g_ev$remove), decreasing = TRUE)[1:25]
top_insert_dates[order(names(top_insert_dates))]
trd_all = data.frame(ht = ifelse(as.numeric(trdat) < 2000, 2000, 
                                 as.numeric(trdat)*0.95),
                     date = as.POSIXct(names(trdat)))
trd = trd_all[c(1:7, 20:25), ]

ggp(ggplot(filter(g_ev %>% filter(event == "life"), remove < max(remove)), aes(remove)) + 
      geom_histogram(
#        data = filter(g_ev, remove < max(remove)),
        data = filter(g_ev, remove >= good_date_remove, remove < max(remove)),
        mapping = aes(remove), binwidth = 60*60*24*365, alpha = 0.2,
        boundary = good_date_remove) +
      geom_histogram(binwidth = 60*60*24, color = "blue") + theme_bw() +
      theme(axis.title.x = element_blank()) + scale_x_datetime(breaks = "1 year"), #+
#      scale_y_continuous(limits = c(0, 9250), breaks = seq(0, 9000, 1000)),
    file = "chronology", width = 10, height = 3)
```
The next analysis looks at *life0* events, plots their frequency, and lists a sample of 10 such events in the raw input data.
```{r}
d2013a = as.POSIXlt.character("2014-05-30 00:00:00 EST")
d2013b = as.POSIXlt.character("2014-06-23 00:00:00 EST")
zeros = g_ev %>% filter(event == "life0" | event == "life", remove > d2013a, remove < d2013b)
ggplot(zeros, aes(remove)) + 
#      geom_histogram(data = zeros,
#        mapping = aes(remove), binwidth = 60*60*24*182.5, alpha = 0.2,
#        boundary = good_date_remove) +
      geom_histogram(binwidth = 60*60, color = "blue") + theme_bw() +
      theme(axis.title.x = element_blank()) + scale_x_datetime(breaks = "1 day") + ylim(c(0, 25))
sn_zeros = zeros$SN[sample(length(zeros$SN), 10)]
for(sn in sn_zeros) print(g_raw[(i = which(g_raw$SN == sn)):(i+10), ])
```

Begin data selection by removing all GPU life times that have *remove* date before *good_date* and all that have zero life times. The premise is that record keeping was different and possibly inconsistent before *r good_date*. **gc_ev_nzg** is used in many analyses going forward. Five original SNs remain after this and are removed manually to have a clean "post 2nd rework cycle" data set. 
```{r}
gc_ev_nzg = gc_ev %>%
  filter(remove > good_date_remove, # drop GPUs removed before good_date!
         !(SN %in% c("0323712022786", "0323712007923", "0323712007994",
                   "0323712022956", "0323712042970")), # rm insert dates < 2013-06
         event != "life0") %>% mutate(event = fct_drop(event))  # remove life0's
last_inventory = max(gc_ev_nzg$remove, na.rm = TRUE) # last right-censoring date
cat("gc_ev dim", dim(gc_ev), "processed into gc_ev_nzg dim", dim(gc_ev_nzg), "\n")
cat("before good_date", sum(gc_ev$remove <= good_date_remove, na.rm = TRUE), ": life0",
    sum(gc_ev$event == "life0", na.rm = TRUE), "remove is NA", sum(is.na(gc_ev$remove)), "\n")
```
**Mark time overlaps within SN, then remove SN records that overlap and plot before-after**. 
SN time overlaps are possible when an inventory is incomplete. A unit is not detected as removed, yet is detected as inserted in a different location - thus the SN appears at both locations until the next inventory.
```{r fig.height = 10}
gc_life = mark.overlaps(gc_ev_nzg %>% filter(event == "life") %>%
                          select(-event), SN)
gc_ev_fail = gc_ev_nzg %>% select(SN, remove, location, event) %>%
  filter(event != "life") %>% arrange(SN, remove, location) %>%
  mutate(event = fct_drop(event))
plot.life_ev(gc_life, gc_ev_nzg, SN, overlaps = TRUE, outs = FALSE, file = "overlap_SN")
gc_life = gc_life %>% filter(!overlap_rec)
plot.life_ev(gc_life, gc_ev_nzg, SN, overlaps = TRUE, outs = FALSE, file = "overlap_SN_after") 
```
For locations, **remove only the overlapping life** but keep the rest of the SN. The overlaps seem very short so this makes sense. Why do location overlaps exist? In some cases, they seem to be the full blade of 4 GPUs. Blades were sometimes hot-swapped by CRAY and if this was during an inventory run, the results could be unpredictable. **Plot overlap at locations again after removing the offending life spans.*
```{r fig.height = 10}
gc_life = mark.overlaps(gc_life, location)
plot.life_ev(gc_life, gc_ev_nzg, location, overlaps = TRUE, outs = FALSE, file = "overlap_Loc")

for(i in which(g_raw$location == "c7-7c0s7n3")) print(g_raw[(i-5):i, ])
for(i in which(g_raw$location == "c22-0c2s0n0")) print(g_raw[(i-5):i, ])
gc_life = gc_life %>% filter(!overlap_rec)
plot.life_ev(gc_life, gc_ev_nzg, location, overlaps = TRUE, outs = FALSE, file = "overlap_Loc_after")
```
After removing overlaps, determine *out* = "last seen" record that may indicate censoring
```{r}
gc_life = gc_life %>% group_by(SN) %>%
  mutate(out = if_else(remove == max(remove) & remove < last_inventory,
                                     TRUE, FALSE)) %>% ungroup()
```
**Inventory interval analysis**
```{r fig.width=10, fig.height=4}
life_insert = gc_life$insert # collect insert dates
life_remove = gc_life$remove # collect remove dates
dates_dbe = sort(unique(filter(gc_ev_fail, event == "DBE")$remove)) # unique DBE dates
dates_otb = sort(unique(filter(gc_ev_fail, event == "OTB")$remove)) # unique OTB dates
dates_life = sort(unique(c(life_insert, life_remove))) # unique insert,remove

life_intervals =
  data.frame(days = as.numeric(diff(dates_life))/(60*60*24),
             year = factor(as.character(year(dates_life[-1])))) %>% filter(days >= 1) %>% 
  mutate(days = factor(as.integer(floor(days)), levels = 1:56))
ggp(ggplot(life_intervals, aes(days, fill = year)) + stat_count() +
      scale_x_discrete(breaks = seq(1, 56, 2), drop = FALSE) + theme_bw(),
    file = "attention_intervals", height = 3)
```

```{r}
nsample = 88 # the max number for a full page pair of figures
set.seed(76513)  # for reproducibility!
fig_height = as.integer(nsample*0.1 + 0.5)
```
**Plot a sample** of `r nsample` GPUs to check that things look right ...
* starts: black dots (gray for zero lifetime)
* lifetimes: black lines
* DBEs: red triangles
* OTB: blue triangles
* Last seen: black ]
```{r}
SN_sample = sample(unique(gc_ev_nzg$SN), nsample)
ev_sample = gc_ev_nzg %>% filter(SN %in% SN_sample)
life_sample = gc_life %>% filter(SN %in% SN_sample)
plot.life_ev(life_sample, ev_sample, SN, "sample_SN", overlaps = FALSE)
```

**Plot a sample** of `r nsample` locations ...
```{r}
loc_sample = sample(unique(gc_ev_nzg$location), nsample)
ev_sample = gc_ev_nzg %>% filter(location %in% loc_sample)
life_sample = gc_life %>% filter(location %in% loc_sample) %>% 
  separate(location, c(NA, "col", "row", "cage", "slot", "node"),
           sep = "[-csn]", convert = TRUE, remove = FALSE) %>%
  mutate(location = reorder(location, col)) # orders in increasing col
plot.life_ev(life_sample, ev_sample, location, "sample_loc", overlaps = FALSE)
```

Add a % uptime in a location histogram. Might be a good diagnostic

Reduce to only "life" records and mark them with ending DBE, OTB, "out", or none events. The result is just *insert* and *remove* with *event* marks to indicate DBE or OTB or "out" (last seen). *bad* indicates that a DBE or OTB occurred yet the GPU was not taken out. For a first cut, keep the *bad* ones in. Also keep *duration* to later aggregate into full life times.

```{r}
gc_life = gc_life %>% select(SN, location, insert, remove, duration, out) %>%
  arrange(SN, remove, location)
gc_fulljoin = gc_life %>% full_join(gc_ev_fail, by = c("SN", "remove", "location"))
gc_full = gc_fulljoin %>% group_by(SN, remove, location) %>%
  mutate(insert = if_else(row_number() > 1, as.POSIXct(NA), insert),
         duration = if_else(row_number() > 1, as.numeric(NA), as.numeric(duration)),
         out = if_else(row_number() > 1, as.logical(NA), out)) %>% ungroup()
```
**Output** gc_full data frame to a CSV file for TBF analysis with Python code
```{r}
gc_write = as.data.frame(lapply(gc_full, as.character), stringsAsFactors = FALSE)
write_csv(gc_write, paste0(out_dir, "gc_full.csv"), na = "", col_names = TRUE)
```
**Message to reader of file:** All fields are character strings. Dates are with timezone. Missing values as "<NA>" strings.

Next, aggregate into one record per *SN* with total life time and first *insert* time. Indicate if event occurred or still in service: out = fail, dbe = fail, otb = fail, NA = right censored (still in service). Add some other variables, such as GPU in single location or moved one or more times, mark *new_batch* group, etc. to use in modeling later.

**This is where life times are determined**, connected with a location and with an event (OTB, DBE, out). Note that location is imprecise if more than one is used (we use longest). Get proportion of time at reference location *time_max_loc*.
```{r}
gc_summary = gc_full %>% arrange(SN, remove) %>% group_by(SN) %>%
  summarize(
    time = sum(duration, na.rm = TRUE), # total life time
    nlife = sum(!is.na(duration)), # number of life records
    nloc = length(unique(location)), # Number of locations where lived
    last = max(remove), # last time stamp
    max_loc = ifelse(!all(is.na(insert)), location[which.max(duration)], ""),
    max_loc_events = ifelse(all(is.na(event)), 0,
                            length(event[location == max_loc])),
    time_max_loc = sum(duration[location == max_loc], na.rm = TRUE)/time,
    dbe = sum(event == "DBE", na.rm = TRUE),
    dbe_loc = ifelse(all(is.na(duration)) | dbe == 0, NA,
                     max_loc %in% location[event == "DBE"]),
    otb = sum(event == "OTB", na.rm = TRUE),
    otb_loc = ifelse(all(is.na(duration)) | otb == 0, NA,
                     max_loc %in% location[event == "OTB"]),
    out = any(out, na.rm = TRUE), # removed
    batch = if_else(min(insert, na.rm = TRUE) >
                      lubridate::mdy_hms("01/01/2016 00:00:00", tz = time_zone), "new", "old")
  )
gc_summary
```

**Separate max_loc** into components (col, row, cage, slot, node). Note that this is the place this *SN* was located the longest. This muddies the hazard analysis somewhat as several locations can be the "treatment" for a given *SN*. But the proportions of time at max_loc tend to be very high. Time-dependent covariates, see: https://cran.r-project.org/web/packages/survival/vignettes/timedep.pdf
```{r}
gc_summary_loc = gc_summary %>% separate(max_loc, c(NA, "col", "row", "cage", "slot", "node"),
           sep = "[-csn]", convert = TRUE, remove = TRUE) %>% # c{col}-{row}c{cage}s{slot}n{node}
  mutate( col = as.factor(col), row = as.factor(row), cage = as.factor(cage), 
          slot = as.factor(slot), node = as.factor(node),
          days = time/(60*60*24),
          years = days/365,
          dead = out & dbe + otb > 0,
          dead_otb = out & otb > 0,
          dead_dbe = out & dbe > 0) # define fail & censored

gc_summary_loc_o = gc_summary_loc %>% filter(batch == "old")
gc_summary_loc_n = gc_summary_loc %>% filter(batch == "new")
gc_summary_loc_censor1 = gc_summary_loc %>% 
  mutate(dead = if_else(batch == "old" & years > 1, FALSE, dead), 
         days = if_else(batch == "old" & years > 1, 1*365, days),
         years = if_else(batch =="old" & years > 1, 1, years))  # censor at 1 year
gc_summary_loc_censor2 = gc_summary_loc %>% 
  mutate(dead = if_else(batch == "old" & years > 2, FALSE, dead), 
         days = if_else(batch == "old" & years > 2, 2*365, days),
         years = if_else(batch =="old" & years > 2, 2, years))  # censor at 2 years
gc_summary_loc_censor3 = gc_summary_loc %>% 
  mutate(dead = if_else(batch == "old" & years > 3, FALSE, dead), 
         days = if_else(batch == "old" & years > 3, 3*365, days),
         years = if_else(batch =="old" & years > 3, 3, years))  # censor at 3 years
gc_summary_loc_censor4 = gc_summary_loc %>% 
  mutate(dead = if_else(batch == "old" & years > 4, FALSE, dead), 
         days = if_else(batch == "old" & years > 4, 4*365, days),
         years = if_else(batch =="old" & years > 4, 4, years))  # censor at 4 years
```
**Output** gc_full to a CSV file
```{r}
gc_write = as.data.frame(lapply(gc_summary_loc, as.character), stringsAsFactors = FALSE)
write_csv(gc_write, paste0(out_dir, "gc_summary_loc.csv"), na = "", col_names = TRUE)
```
Get some stats for sanity check.
```{r}
cat("Total", nrow(gc_summary_loc), "unique lifetimes\n")
cat("  Censored:", sum(!gc_summary_loc$dead), "  dead:", sum(gc_summary_loc$dead),
    "  Known alive:", sum(!gc_summary_loc$out), "\n")
cat("  DBE or OTB during life:", sum(gc_summary_loc$dbe > 0 | gc_summary_loc$otb >0), "\n")
cat("           More than one:", sum(gc_summary_loc$dbe + gc_summary_loc$otb > 1, na.rm = TRUE), "\n")
ggplot(gc_summary_loc, aes(x = time)) + geom_histogram(bins = 60) + ggtitle("GPU Life Times")
ggplot(gc_summary_loc, aes(x = time)) + geom_histogram(bins = 60) + 
  facet_wrap(~ batch) + ggtitle(paste("GPU Life Times - new and old"))
ggplot(gc_summary_loc, aes(x = time_max_loc)) + geom_histogram(bins = 200) +
  ggtitle("Life Proportion at Longest Location") + theme_bw()
ggplot((gc_summary_loc %>% filter(dead)), aes(x = time_max_loc)) +
      geom_histogram(bins = 200) + theme_bw() +
      ggtitle("Life Proportion at Longest Location - Dead Only")
```

Kaplan-Meier survival analysis gives the probability of survival beyond a given time. It is a nonparametric technique that makes no specific failure model assumptions, such as Weibull, Exponential, etc. It can also split the data into subpopulations and compute separate survival curves.

If $T$ is taken to be the random variable of a GPU fail time, then its probability density function is $f(t)$ and its cumulative distribution function $F(t) = Pr\{T < t\}$ gives the probability that a GPU fails by duration $t$. The survival function is then
$$S(t) = Pr\{T \geq t\} = 1 - F(t) = \int_t^\infty f(x)dx,$$ 
which is the probability of being operational just before duration $t$.

A sister concept to the survival function is the hazard function:
$$H(t) = \frac{f(t)}{S(t)},$$
which is what is sometimes called the "bathtub" curve in reliability.

Units removed without a DBE are right-censored! They are not failures!

Next, we consider splitting the GPU into groups - installed before 2016 (old) or later (new).

**SN categories**
I tried disassembling the *SN* into digits and did not find any compelling results.

Now for Kaplan-Meier analysis ...
```{r fig.height = 8}
library(survival)
library(survminer)
ggp(ggsurvplot(survfit(
  Surv(years, dead, type = 'right') ~ node, data = gc_summary_loc),
  facet.by = c("batch", "cage"), conf.int = TRUE, risk.table = TRUE,
  ggtheme = theme_bw(), xlab = "Years", legend.title = "Node", censor.size = 2,
  legend = c(0.08, 0.73), legend.labs = 0:3, ylim = c(0.08, 1), size = 0.4, 
  break.y.by = 0.1, censor = FALSE),
  width = 10.5, height = 6, file = "km_cage-node_a")
```
Consider diagnostics for proportionalty.
```{r}
fit = coxph(Surv(years, dead, type = 'right') ~ row + col + cage + slot + node,
            data = gc_summary_loc_o)
## below seems to have an issue with RStudio: 
## https://community.rstudio.com/t/ggcoxdiagnostics-warning-message/51867
## ggcoxdiagnostics(fit, type = "schoenfeld", ox.scale = "time")
test.ph = cox.zph(fit)
ggcoxzph(test.ph)
```
The Cox proportional hazards (CPH) regression analys takes the hazard function above and a set of covariates $x$ and models $k$s GPU hazard as
$$H_k(t) = H_0(t)e^{\sum\limits_{i=1}^n{\beta x}}$$
That is, a base hazard rate multiplied by a function of covariates. The key assumption in the CPH model is that the "hazards" are multipliers on the "baseline hazard" but the estimate of the baseline hazard is nonparametric, meaning it makes no specific distributional assumption and just learns from the data (unlike analyses that assume for example a Weibul or an exponential model). The multiplicative assumption is that the hazard curves do not cross and are multiples of each other. The multiplier is the *hazard coefficient*. For example, if the baseline is *node0* and the *node2* hazard is 2, then *node2* sees twice as many failures as *node0* on average.

For our comparison of hazards, we use as *covariates* all the unpacked location variables, indicators whether the GPU has been moved, and our notion of old and new GPUs based on first install date. Some GPUs experience time at more than one location but these are few. We use the longest period location in this analysis. We do separate analysis for old and new batches.
```{r fig.width = 5, fig.height = 10}
response = "Surv(time, event = dead) ~"
gc_summary_loc_o = gc_summary_loc %>% filter(batch == "old")
gc_summary_loc_n = gc_summary_loc %>% filter(batch == "new")

col2torus = order(c(0, seq(1, 23, 2), seq(24, 2, -2)))
library(forestmodel)
panels = list(
  forest_panel(width = 0.03),
  forest_panel(width = 0.10, display = variable, fontface = "bold",
               heading = "Variable"),
  forest_panel(width = 0.05, display = level, fontface = "bold"),
  forest_panel(width = 0.05, display = n, heading = "N"),
  forest_panel(width = 0.05, display = n_events, heading = "Events"),
  forest_panel(width = 0.03, item = "vline", hjust = 0.5),
  forest_panel(width = 0.91, item = "forest", line_x = 0, hjust = 0.5,
               heading = "Hazard ratio", linetype = "dashed"),
  forest_panel(width = 0.03))

covariates = c("col", "row", "cage", "slot", "node")
levels(gc_summary_loc_o$col) = 
  paste0(formatC(levels(gc_summary_loc_o$col), width = 2),
         "   (X-", formatC(col2torus, width = 2), ")")
mv_model = coxph(as.formula(paste(response, paste(covariates, collapse = " + "))),
                 data = gc_summary_loc_o)
ggp(forest_model(mv_model, panels, factor_separate_line = TRUE), width = 5,
    height = 7, file = "cox_o")

tcol = col2torus[gc_summary_loc_o$col]
gc_summary_loc_o = gc_summary_loc_o %>% mutate(col = reorder(col, tcol))
mv_model = coxph(as.formula(paste(response, paste(covariates, collapse = " + "))),
                 data = gc_summary_loc_o)
ggp(forest_model(mv_model, panels, factor_separate_line = TRUE), width = 5,
    height = 7, file = "cox_o_t")

levels(gc_summary_loc_n$col) = 
  paste0(formatC(levels(gc_summary_loc_n$col), width = 2),
         "   (X-", formatC(col2torus, width = 2), ")")
mv_model = coxph(as.formula(paste(response, paste(covariates, collapse = " + "))),
                 data = gc_summary_loc_n)
ggp(forest_model(mv_model, panels, factor_separate_line = TRUE), width = 5,
    height = 7, file = "cox_n")

tcol = col2torus[gc_summary_loc_n$col]
gc_summary_loc_n = gc_summary_loc_n %>% mutate(col = reorder(col, tcol))
mv_model = coxph(as.formula(paste(response, paste(covariates, collapse = " + "))),
                 data = gc_summary_loc_n)
ggp(forest_model(mv_model, panels, factor_separate_line = TRUE), width = 5,
    height = 7, file = "cox_n_t")
```


```{r}
cat("Total time:\n"); print(proc.time() - Rmd.time)
```