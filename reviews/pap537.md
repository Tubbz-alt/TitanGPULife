
# Reviewer 1

## Detailed Comments for Authors

This paper reports on the reliability of the GPUs used in the Titan
supercomputer for the 2014-2019 time period. 

Detailed comments:
 
I.
- What is meant with “data collection changes” and how are they
  responsible for GPU swaps? 
  * *Done: Reworded third paragraph to clarify*
- Rephrase the sentence: “TBF studies largely how…” -> “TBF studies
  largely show how…”
  * *Done*

II.
- Typo: d0 -> do
  * *Done*

III.
- You should explain in more detail the “double bit error” and “off
  the bus” failure events. 
  * *Done: See first paragraph of III*
- What are the causes for “system boots”? I assume it’s not just event
  failures.
  * *Done: See second paragraph of III.*
- What conclusions should one draw from the inter-inventory time shown
  in Fig. 2? I assume there are different reasons for system boots,
  so, I’m not clear how to interpret this graph.
  * *Done: See second paragraph of III*
- Why would you record inventory independently of system boots? I
  assume inventory can only change after a system boot.
  * *Done: See second paragraph of III. Warm swaps.*

V.
- Shouldn’t graphs be separated for the two batches (pre/post the 2017
  replacement)? As you report, failure characteristics look different
  with the post 2017 batch being more reliable. For example, don’t the
  MTBFs in Fig. 7 look different for the two batches?
  * *Done: See updated Fig. (split old-new and units of years) and text in second paragraph of V. 
     Rizwan: split old-new and make MTBF unit years*
- With respect to failure events, what triggers the retirement or end
  of life of a GPU?
  * *See second paragraph of VI*
- Fig. 10: How big are the partitions with old and new GPUs,
  respectively?
  * *Done: See updated Fig. with size of new partition plotted on secondary y-axis.
     Some text was also updated in the second last paragraph of V.  
     Rizwan: add info to graph*

VI.
- Can you please add references for all analysis methods mentioned in
  this section, e.g. for Kaplan-Meier?
  * *Done.*
- Can you please explicitly label nodes 0-3 and slots 0-7 in Fig. 4 to
  make it easier for the reader to interpret location-dependent
  results.
  * *Done*
- Fig. 12/13: I assume the figures show the mean and std (or median,
  Q1, Q3)?
  * *Done: they are 95% confidence intervals for the mean on log scale.*
- There is a reference missing on page 9 [?].
  * *Done.*
- Typo: “has mot” -> “has not”
  * *George: Done.*

VII.
- Can you quantify how changing job scheduling improved productivity?
  * *Christian: is there a paper reference about this?*
- Is there any data available on the reliability of other system
  components (CPUs, DRAM, interconnect, etc.)?
  * *Done: see first paragraph of III. Yes, but not exceptional.*


## Comments for Revision

- I suggest showing separate graphs for the old and new GPU batches,
  e.g. in Fig. 2 and Fig. 7. 
  * *Not relevant to Fig 2. Figure 2 speaks to inventory times, which
    were the same for old and new batches. Done: Updated Fig. Rizwan: Fig 7*
- Your data shows a clear correlation between reliability and
  location. Do you have location-specific operating conditions and
  environmental data available to strengthen this point?
  * *Done: see sentence starting with "An increasing temoerature ...*

# Reviewer 2

## Detailed Comments for Authors

This paper presents curated data derived from periodic logs regarding
the lifetimes of the GPU components on the Titan
supercomputer. Analysis of faults, repairs, and reconfiguration over
roughly five years of operation are presented. While the data that has
been collected and presented is of great value, some additional
clarifications and explanations are needed for understanding how the
authors drew particular conclusions in this paper.

### Further comments:
- Section 3: are DBE and OTB the only failures events logged? Are
  there no other failure modes of relevance oradditional conditions
  that may have triggered a replacement?
  * *Done: see first paragraph of III*
- Fig. 2: a square root scale is hard to parse and having the lowest
  y-axis mark at 500 isn't particularly useful. It would help to add
  further tick marks.
  * *Removed square root scale.*
    *In addition the graph was using the wrong subset of data, which
    is now corrected.* 
- Fig. 2: based on the text, the weekly inventory condition should
  appear on this graph for later years; however, this does not appear
  to be the case (i.e., around the 7 day mark, only years before 2015
  seem to appear). If system boots were rare in the latter years,
  which necessitated this weekly inventory, the chart is not
  reflecting this reality. It would be helpful to clarify.
  * *The weekly reboots were implemented at some point in 2016 after
	realizing the sparsity of the earlier data.* 
	*However reboots due to
    failures started to increase about the same time, making this addition
    moot.*
- Section 4: it would be helpful to explain the reason for overlapping
  logs (GPU life and location). Is this due to some logging
  peculiarity?
  * *Overlapping GPU life can occur due to incomplete inventory*
- Section 6: how was the conclusion that GPU lifetimes were
  correlating to the cooling architecture reached? Was this validated
  against some other logs (e.g., temperature or chiller performance
  logs)?
  * *Improved Figure 4. Also, found temperature confirmation.*
- It would be helpful to correlate lifetime data against any
  utilization or scheduler logs as well. It is unclear if GPU
  utilization was uniform (which the text seems to indicate was not
  the case due to the adaptive scheduler). Unexpected job
  scheduling/mapping could also contribute to peculiarities in the
  data. It would be helpful to rule out this scenario as an effect.
  * *We did find that scheduling contributed to the patterns*
  *Interleaving hazard values correlate with small job scheduling fill and
  toroidal folding.*
- While overall readability was not substantially impacted, it would
  be helpful to address the various minor grammatical issues (mostly
  centered around the first 2-3 pages).
  * *Some done and continuing.*

## Comments for Revision

- The main revision that should be made is additional discussion
  regarding how the conclusion that system cooling was having a large
  impact on GPU lifetime was being reached. While plausible, further
  validation or confirmation via additional logs or analysis would be
  helpful. Additionally, it would be helpful to rule out other
  contributing factors such as poor scheduling or utilization.
  * *We have added temperature data confirmation reported in another
    study as well as contributing scheduling confirmation.*

# Reviewer 3

## Detailed Comments for Authors

- I enjoyed reading the paper and I think it fits well in the “State
  of the Practice” area. It is well written and organized, with a good
  selection of figures used for the analysis. I appreciate the efforts
  made by the authors to highlight the kind of problems that
  world-class facilities and top 10 supercomputers can face over time
  and their wiliness to learn what happen and how could it be avoided
  in the future. The analysis of data gathered over so many years
  provides a useful view from a higher level. Considering the
  relatively short lifetime of these machines, studies that spam data
  of the whole (or close to) whole lifetime are very valuable. As the
  authors mentioned, not all the lessons learned will apply broadly to
  other HPC centers but most of them will and, since the size of
  facilities keeps growing this study can improve their design and
  prepare them for a better operation.

- The authors were able to clearly identify issues and explain them in
  a comprehensive manner. The section “Related Work” at the beginning
  of the document helps to set a good base for the analysis carried
  out in the following sections and contains relevant and sufficient
  citations. All the figures were very useful and curated. The
  insights from this article will benefit facility operators, users
  and application developers for more proactive evaluations of the
  system and improvements in scheduling and checkpointing techniques,
  among others. In addition, the intention from the authors to make
  publicly available the codes to reproduce the graphics and the GPU
  data from Titan is highly appreciated.

- I find very intriguing the row pattern that you mentioned at the end
  of section VI and present in the old batch in Fig. 12. Not required
  for revision but I would encourage the authors to dig in more. Is
  this due to external factors (different heat/cool patterns in the
  machine room)? Where are cables coming/going, is this due to
  distances in any component? Are the walls or empty spaces
  responsible for it? Do you see this in Summit now or could be
  specific to Titan? Do you have any outliers in the data causing this
  effect? If you take the cage component or the timeline/year, do you
  see the same pattern? E.g. old batch were “new” components at a
  certain point. I would push the analysis and discuss with the
  Operations team and the vendors. Same thing with the 0-10 column
  zig-zag pattern. Any data from internal counters that would be
  useful to discard or confirm hypothesis?
  * *George: zig-zag fits toroidal foldover and is explainable with
    scheduling*

### Figures: 

The figures presented are very useful to guide the analysis described
in the text. However, some of them would benefit from additional
clarifications. See suggestions below:

- Figure 1: it would be useful to add the numbers to some of the main
  peaks in the figure.
- New: It would be useful to have a figure similar to Fig. 1 but with
  the cumulative data per year for the whole 7 years of life to see
  total and annual number of units replaced. A table would also work.
  *  *Modified the figure to accomplish both of the above. The ealy
     tall bar with 18K: is it an artifact or did all the GPU move?*
- Figure 3: The text in Fig. 3 is really too small. At least, I
  suggest trying to use the entire width of the column.
  * *George: fixed*
- Figure 3: Would it be useful to include the time zone while talking
  about this figure -mainly to know if the raw data always pertain to
  the same time zone or if they change as mentioned in section IV.
  * *Don: are all times ET and change for daylight time (EST-EDT)?*
- Figure 4: I would personally add one of the sequences
  (e.g. c17-4c1s3n1) in the white space in the middle-right part of
  the image with red arrows pointing from the exact location of the
  component in the different sections (columns, rows, etc.) to the
  corresponding images. That would help make the connection with the 4
  images and raw data described in that section.
  * *George: Done*
- Figure 5: Despite the fact that data are random, I would order the
  Y-axis based on location (c0-c24)
  * *George: Done*
- Figures 5 and 6: Since the article has room, I would expand a little
  bit in the Y-axis Figure 5 and 6 to allow clarify the location and
  SN raw data. Also, why 60 entries in Fig. 5 and 80 entries in
  Fig. 6? Why did not you use the same number? is this multiple of any
  number? E.g. for location, 80 corresponds to 1/X of the total, or
  something like that? Just a way to guide the user who would like to
  apply this visualization to their own data.
  * *George: They're both 60. We missed correcting the 80 in the text as we
    tried to latex-float the figure to the right place.*
- Figures 5 and 6: There are lines that do not finish with ], OTB or
  DBE events. They just end. Example: 0323812005773 in fig. 5. Is that
  because they were part of another board in which something went
  wrong but they are in reality ok and can be reused?
  * *George: Yes. That particular GPU was in 3 locations and still
    operating at the end. It's life is "censored".*
- Figure 7: If you look closely at Fig. 7 we can see it does not have
  the bars exactly aligned in the X-axis. Any main reason for that? It
  would be helpful to report data in two-columns rather than in just
  one. Also, each bar seems to represent the equivalent of 3 weeks, is
  that correct? If the figure is expanded, it would be interesting to
  use more bars (e.g. 2 weeks) and see the differences in the peaks. 2
  weeks is also more intuitive (= half a month, can be space between
  maintenance periods, etc.)
  * *Done: Updated Fig. 7 to have separate columns for DBE and OTB events,
     each bar now represents 2 weeks, and converted to years as units.  
     Rizwan: Also, converting to years as units would be more
    interpretable.* 

### Typographical errors:
1. pp. 1, Sec. I, col 2: “desctibed” -> “described”;
2. pp. 1, Sec. I, col 2: “wheras” -> “whereas”;
3. pp. 2, Sec. II, col 1: “d0 not” -> “do not”;
4. pp. 3, Sec. III, col 2: “intalled” -> “installed”;
5. pp. 4, Sec. IV, col 2: “stil” -> “still”;
6. pp. 6, Sec. V, col 2: “partitio” -> “partition”;
7. pp. 9, Sec. VI, col 1: “mot” -> “not”; 
8. pp. 9, Sec. VI, col 2: “[?]” -> “[Missing reference]”; 
9. pp. 10, Sec. VII, col 1: “Saparating” -> “Separating”.
   * *George: All done.*

### Other:
1. the acronym “(NCSA)” is included but never used. Please review that
   all the acronyms are used and if not delete them;
   * *George: removed. Do we mark it with strikethrough?*
2. the acronym “(SN)” is included but figures 5 & 6 (and the text in
   page 5) use “sn” instead. Would be good to use the acronym instead;
   * *George: done.*
3. the acronym of Survival Analysis could be placed 4 lines before, at
   the moment of the 1st reference to it.
   * *George: done.*
4. Try to follow conventions for references to figures, i.e. beginning
   of sentence “Figure X”, in the text “Fig. X”, for instance, revisit
   reference to “Figure 4” in page 3 and “Figure 7” in page 5.
   * *George: done.*
5. I did not find any acknowledgement section. Not sure if this is
   missing or side-effect of double-blind review.
   * *George: partly revealed now.*

## Comments for Revision

The article already provides an in-depth analysis. However, it would
be enhanced if the following points are considered in a revised
version:

###  Major points
- How the experience generalized to wider applicability should be
  explored further. It would be useful to expand the discussion
  section and relate this with other HPC centers of different size,
  different machine architecture and characteristics (smaller racks or
  less complicated cabinet structure). For instance, could this
  analysis be applied more frequently in smaller machines with longer
  lifetime? How can these techniques or visualization methods scale to
  pre-exascale and exascale machines? Cost and human effort are
  mentioned in the final discussion but what is the impact in science?
  i.e. computing hours lost due to failures that could have been used
  for science, any estimation of that amount? Any comment about what
  to do in those cases, will the core-hours be refunded to
  projects/allocations? How easy would be to integrate this analysis
  and visualizations into Operational Data Analytics frameworks? Are
  you using this now on Summit and will you use and improve it for
  Frontier? If a facility does not have the same initial data, how
  much effort can take the adapt the scripts to other HPC raw
  data. Section VI would benefit from some references related to
  studies of SA methods.
  * *Architects don't always consider non-uniform airflow and
    resulting impact on components. Leadership architectures (Summit,
    Frontier) are already better. Smaller clusters don't have the
    luxury of large number of units on test and consequently failures
    appear random (wide confidence intervals and statistically not
    significant).*
	
  *	*We are already discussing implementing some of these analyses and
    visualizations for data from Summit and Frontier. We also have
    better data collection implemented. (doubleblind this?)*
	
  * *Christian/Don/Jim: comment of user impact and core-hour refunds?*

- It would be useful to emphasize more the novelty in this article. It
  seems that the main elements are the full lifetime data analysis and
  the atypical failure mode analysis, with the codes that allow
  visualization but it is not clear how much and which graphical
  representations are new (what particular figures). It would be
  useful to clarify that. Is there a recommended order for the
  visualization or full lifetime analysis they proposed that would be
  useful? Does the tool provide a new dashboard or are the graphs
  created/visualized independently? Any feedback loop or
  steps/flowchart used in the analysis in case the order presented in
  the article is not the same that would naturally help to detect
  issues? Will you run into memory, storage or visualization issues
  with the current codes used? What are the current limitations of the
  codes (e.g. speed, ease of use, tutorial or guidelines to reproduce
  graphs) (if any) or dependencies with other software?
  * *Figures 1, 2, 5, 6 are custom using R's ggplot2*
	*Figures 11, 12, and 13 are customized variants from R packages
    \pkg{survminer} and \pkg{forestmodel}, which also use ggplot2.*

# Reviewer 4

## Detailed Comments for Authors

- The paper presents a reliability analysis of the Titan
  SuperComputer. The authors perform time between failures analysis
  and statistical survival analysis on operational datasets that also
  are publicly made available. The main finding appears that GPU
  reliability is dependent on heat dissipation to an extent that
  strongly correlates with detailed nuances of the system cooling
  architecture.

- The paper presents a valuable analysis on a real dataset. It is
  definitely positive that datasets and methodologies are openly
  provided for further analysis. The main findings are mostly
  positional, and thus correlated with airflow. Are there more
  quantitative values that can can further reinforce this correlation?
  I.e., for differences in col 0-11 from col 12-24, while there is the
  speculation of different flows or pressure, the authors also report
  temperature.
  * *Looking for connection with torus foldover. Do we have
    temperature data?*

- The last part of the analysis seems written quickly, and could be
  better streamlined. There also are a few typos. The paper seems to
  be missing a proper conclusion, which is embedded in the
  discussion. One question, given the facility involved in this
  retrospective, is if the analysis has influenced somehow the current
  and the upcoming (GPU based) systems.
  * *Jim, Don: What are we taking to Summit and Frontier?*
  * *Done: Christian: add a Conclusions section?*


## Comments for Revision

- Please improve the quality of the last part of the analysis
- Please provide where possible some more qualitative values related
  to issues with airflow/temperature

- Please see if it possible to provide a proper conclusion.
* *See notes above in Detailed Comments*
