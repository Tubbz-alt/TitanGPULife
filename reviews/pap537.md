  * *Christian: makefile mod for pandoc?*

# Reviewer 1

## Detailed Comments for Authors

This paper reports on the reliability of the GPUs used in the Titan
supercomputer for the 2014-2019 time period. 

Detailed comments:
 
I.
- What is meant with “data collection changes” and how are they
  responsible for GPU swaps? 
  * *Don: can you elaborate on data collection prior to 2014?
    ... especially regarding the full machine spike.*
	*May 2013 acceptance?*
- Rephrase the sentence: “TBF studies largely how…” -> “TBF studies
  largely show how…”
  * *George: Done*

II.
- Typo: d0 -> do
  * *George: Done*

III.
- You should explain in more detail the “double bit error” and “off
  the bus” failure events. 
  * *Don? GDDR5, #pcie lanes available to card. pcie2!=8 (4, 2)*
- What are the causes for “system boots”? I assume it’s not just event
  failures.
  * *Don? We rebooted as often as weekly to do software updates
    patches etc.*
- What conclusions should one draw from the inter-inventory time shown
  in Fig. 2? I assume there are different reasons for system boots,
  so, I’m not clear how to interpret this graph.
  * *George: had event dates mixed in before. Also stacking a
    histogram uner a y axis transform is a "bug" in ggplot2.*
	*I expected remove dates to reveal inventory frequency but this
    does not work. Why?*
	*If a reboot takes about a minute, all insert and remove time
    stamps should be withi a minute interval. But this is not the case.*
  
	*Warm swaps*
	*Toward the end - daily inventories*
	*2017 - started daily inventories*
	*check sqrt axis???*
- Why would you record inventory independently of system boots? I
  assume inventory can only change after a system boot.
  * *Don: system boots can be far apart while blades with GPUs can be
    hot-swapped?* 
	*Also, see above comment.* 

V.
- Shouldn’t graphs be separated for the two batches (pre/post the 2017
  replacement)? As you report, failure characteristics look different
  with the post 2017 batch being more reliable. For example, don’t the
  MTBFs in Fig. 7 look different for the two batches?
  * *Rizwan: split old-new and make MTBF unit years*
- With respect to failure events, what triggers the retirement or end
  of life of a GPU?
  * *Don: why is OTB or DBE end of life?*
  * *First: DBE not end. Losing memory pages 64? Eventually figured
     that OTB and DBE would not go away*
- Fig. 10: How big are the partitions with old and new GPUs,
  respectively?
  * *Rizwan: add info to graph*

VI.
- Can you please add references for all analysis methods mentioned in
  this section, e.g. for Kaplan-Meier?
  * *George: started this task. More is needed.*
- Can you please explicitly label nodes 0-3 and slots 0-7 in Fig. 4 to
  make it easier for the reader to interpret location-dependent
  results.
  * *George: Done*
- Fig. 12/13: I assume the figures show the mean and std (or median,
  Q1, Q3)?
  * *George*
- There is a reference missing on page 9 [?].
  * *George*
- Typo: “has mot” -> “has not”
  * *George: Done*

VII.
- Can you quantify how changing job scheduling improved productivity?
  * *Christian: is there a paper reference about this?*
- Is there any data available on the reliability of other system
  components (CPUs, DRAM, interconnect, etc.)?
  * *Don: ? Failure rates external to SXM module followed expected
    MTTF etc.*


## Comments for Revision

- I suggest showing separate graphs for the old and new GPU batches,
  e.g. in Fig. 2 and Fig. 7. 
  * *Not relevant to Fig 2. Rizwan: Fig 7*
- Your data shows a clear correlation between reliability and
  location. Do you have location-specific operating conditions and
  environmental data available to strengthen this point?
  * *See Reviewer 2 Utilization*
   *See papers by Devesh and by Zimmer.*

# Reviewer 2

## Detailed Comments for Authors

This paper presents curated data derived from periodic logs regarding
the lifetimes of the GPU components on the Titan
supercomputer. Analysis of faults, repairs, and reconfiguration over
roughly five years of operation are presented. While the data that has
been collected and presented is of great value, some additional
clarifications and explanations are needed for understanding how the
authors drew particular conclusions in this paper.

### Further comments:
- Section 3: are DBE and OTB the only failures events logged? Are
  there no other failure modes of relevance oradditional conditions
  that may have triggered a replacement?
  * *Don: Other failures were much less frequent and were not
    correlated with the failing resistor issue, which was the driving
    cause of GPU board replacement.*
- Fig. 2: a square root scale is hard to parse and having the lowest
  y-axis mark at 500 isn't particularly useful. It would help to add
  further tick marks.
  * *George: See Reviewer 1 on Fig 2*
- Fig. 2: based on the text, the weekly inventory condition should
  appear on this graph for later years; however, this does not appear
  to be the case (i.e., around the 7 day mark, only years before 2015
  seem to appear). If system boots were rare in the latter years,
  which necessitated this weekly inventory, the chart is not
  reflecting this reality. It would be helpful to clarify.
  * *George: See Reviewer 1 on Fig 2*
- Section 4: it would be helpful to explain the reason for overlapping
  logs (GPU life and location). Is this due to some logging
  peculiarity?
  * *George: See Reviewer 1 on Fig 2*
- Section 6: how was the conclusion that GPU lifetimes were
  correlating to the cooling architecture reached? Was this validated
  against some other logs (e.g., temperature or chiller performance
  logs)?
  * *George: Improved Figure 4.*
  *See papers by Devesh and by Zimmer.*
- It would be helpful to correlate lifetime data against any
  utilization or scheduler logs as well. It is unclear if GPU
  utilization was uniform (which the text seems to indicate was not
  the case due to the adaptive scheduler). Unexpected job
  scheduling/mapping could also contribute to peculiarities in the
  data. It would be helpful to rule out this scenario as an effect.
  * *Utilization:*
  *Interleaving hazard values correlate with small job scheduling fill and
  toroidal folding. George: Get the details to work out.*
- While overall readability was not substantially impacted, it would
  be helpful to address the various minor grammatical issues (mostly
  centered around the first 2-3 pages).
  * *George: Some done and continuing.*

## Comments for Revision

- The main revision that should be made is additional discussion
  regarding how the conclusion that system cooling was having a large
  impact on GPU lifetime was being reached. While plausible, further
  validation or confirmation via additional logs or analysis would be
  helpful. Additionally, it would be helpful to rule out other
  contributing factors such as poor scheduling or utilization.
  * *See Utilization above*

# Reviewer 3

## Detailed Comments for Authors

- I enjoyed reading the paper and I think it fits well in the “State
  of the Practice” area. It is well written and organized, with a good
  selection of figures used for the analysis. I appreciate the efforts
  made by the authors to highlight the kind of problems that
  world-class facilities and top 10 supercomputers can face over time
  and their wiliness to learn what happen and how could it be avoided
  in the future. The analysis of data gathered over so many years
  provides a useful view from a higher level. Considering the
  relatively short lifetime of these machines, studies that spam data
  of the whole (or close to) whole lifetime are very valuable. As the
  authors mentioned, not all the lessons learned will apply broadly to
  other HPC centers but most of them will and, since the size of
  facilities keeps growing this study can improve their design and
  prepare them for a better operation.

- The authors were able to clearly identify issues and explain them in
  a comprehensive manner. The section “Related Work” at the beginning
  of the document helps to set a good base for the analysis carried
  out in the following sections and contains relevant and sufficient
  citations. All the figures were very useful and curated. The
  insights from this article will benefit facility operators, users
  and application developers for more proactive evaluations of the
  system and improvements in scheduling and checkpointing techniques,
  among others. In addition, the intention from the authors to make
  publicly available the codes to reproduce the graphics and the GPU
  data from Titan is highly appreciated.

- I find very intriguing the row pattern that you mentioned at the end
  of section VI and present in the old batch in Fig. 12. Not required
  for revision but I would encourage the authors to dig in more. Is
  this due to external factors (different heat/cool patterns in the
  machine room)? Where are cables coming/going, is this due to
  distances in any component? Are the walls or empty spaces
  responsible for it? Do you see this in Summit now or could be
  specific to Titan? Do you have any outliers in the data causing this
  effect? If you take the cage component or the timeline/year, do you
  see the same pattern? E.g. old batch were “new” components at a
  certain point. I would push the analysis and discuss with the
  Operations team and the vendors. Same thing with the 0-10 column
  zig-zag pattern. Any data from internal counters that would be
  useful to discard or confirm hypothesis?
  * *George: follow up zig-zag with toroidal foldover*
    *See if it persists for subsets of year or subsets of cages.*

### Figures: 

The figures presented are very useful to guide the analysis described
in the text. However, some of them would benefit from additional
clarifications. See suggestions below:

- Figure 1: it would be useful to add the numbers to some of the main
  peaks in the figure.
- New: It would be useful to have a figure similar to Fig. 1 but with
  the cumulative data per year for the whole 7 years of life to see
  total and annual number of units replaced. A table would also work.
  *  *Modified the figure to accomplish both of the above. The ealy
     tall bar with 18K: is it an artifact or did all the GPU move?*
- Figure 3: The text in Fig. 3 is really too small. At least, I
  suggest trying to use the entire width of the column.
  * *George: fixed*
- Figure 3: Would it be useful to include the time zone while talking
  about this figure -mainly to know if the raw data always pertain to
  the same time zone or if they change as mentioned in section IV.
  * *Don: are all times ET and change for daylight time (EST-EDT)?*
- Figure 4: I would personally add one of the sequences
  (e.g. c17-4c1s3n1) in the white space in the middle-right part of
  the image with red arrows pointing from the exact location of the
  component in the different sections (columns, rows, etc.) to the
  corresponding images. That would help make the connection with the 4
  images and raw data described in that section.
  * *George: Done*
- Figure 5: Despite the fact that data are random, I would order the
  Y-axis based on location (c0-c24)
- Figures 5 and 6: Since the article has room, I would expand a little
  bit in the Y-axis Figure 5 and 6 to allow clarify the location and
  SN raw data. Also, why 60 entries in Fig. 5 and 80 entries in
  Fig. 6? Why did not you use the same number? is this multiple of any
  number? E.g. for location, 80 corresponds to 1/X of the total, or
  something like that? Just a way to guide the user who would like to
  apply this visualization to their own data.
  * *We missed correcting the 80 in the text as we tried to float the
    figure to the right place.*
- Figures 5 and 6: There are lines that do not finish with ], OTB or
  DBE events. They just end. Example: 0323812005773 in fig. 5. Is that
  because they were part of another board in which something went
  wrong but they are in reality ok and can be reused?
- Figure 7: If you look closely at Fig. 7 we can see it does not have
  the bars exactly aligned in the X-axis. Any main reason for that? It
  would be helpful to report data in two-columns rather than in just
  one. Also, each bar seems to represent the equivalent of 3 weeks, is
  that correct? If the figure is expanded, it would be interesting to
  use more bars (e.g. 2 weeks) and see the differences in the peaks. 2
  weeks is also more intuitive (= half a month, can be space between
  maintenance periods, etc.)

### Typographical errors: *Done except for reference*
1. pp. 1, Sec. I, col 2: “desctibed” -> “described”;
2. pp. 1, Sec. I, col 2: “wheras” -> “whereas”;
3. pp. 2, Sec. II, col 1: “d0 not” -> “do not”;
4. pp. 3, Sec. III, col 2: “intalled” -> “installed”;
5. pp. 4, Sec. IV, col 2: “stil” -> “still”;
6. pp. 6, Sec. V, col 2: “partitio” -> “partition”;
7. pp. 9, Sec. VI, col 1: “mot” -> “not”; 
8. pp. 9, Sec. VI, col 2: “[?]” -> “[Missing reference]”; 
9. pp. 10, Sec. VII, col 1: “Saparating” -> “Separating”.

### Other:
1. the acronym “(NCSA)” is included but never used. Please review that
   all the acronyms are used and if not delete them;
2. the acronym “(SN)” is included but figures 5 & 6 (and the text in
   page 5) use “sn” instead. Would be good to use the acronym instead;
3. the acronym of Survival Analysis could be placed 4 lines before, at
   the moment of the 1st reference to it.
4. Try to follow conventions for references to figures, i.e. beginning
   of sentence “Figure X”, in the text “Fig. X”, for instance, revisit
   reference to “Figure 4” in page 3 and “Figure 7” in page 5.
5. I did not find any acknowledgement section. Not sure if this is
   missing or side-effect of double-blind review.

## Comments for Revision

The article already provides an in-depth analysis. However, it would
be enhanced if the following points are considered in a revised
version:

###  Major points
- How the experience generalized to wider applicability should be
  explored further. It would be useful to expand the discussion
  section and relate this with other HPC centers of different size,
  different machine architecture and characteristics (smaller racks or
  less complicated cabinet structure). For instance, could this
  analysis be applied more frequently in smaller machines with longer
  lifetime? How can these techniques or visualization methods scale to
  pre-exascale and exascale machines? Cost and human effort are
  mentioned in the final discussion but what is the impact in science?
  i.e. computing hours lost due to failures that could have been used
  for science, any estimation of that amount? Any comment about what
  to do in those cases, will the core-hours be refunded to
  projects/allocations? How easy would be to integrate this analysis
  and visualizations into Operational Data Analytics frameworks? Are
  you using this now on Summit and will you use and improve it for
  Frontier? If a facility does not have the same initial data, how
  much effort can take the adapt the scripts to other HPC raw
  data. Section VI would benefit from some references related to
  studies of SA methods.

- It would be useful to emphasize more the novelty in this article. It
  seems that the main elements are the full lifetime data analysis and
  the atypical failure mode analysis, with the codes that allow
  visualization but it is not clear how much and which graphical
  representations are new (what particular figures). It would be
  useful to clarify that. Is there a recommended order for the
  visualization or full lifetime analysis they proposed that would be
  useful? Does the tool provide a new dashboard or are the graphs
  created/visualized independently? Any feedback loop or
  steps/flowchart used in the analysis in case the order presented in
  the article is not the same that would naturally help to detect
  issues? Will you run into memory, storage or visualization issues
  with the current codes used? What are the current limitations of the
  codes (e.g. speed, ease of use, tutorial or guidelines to reproduce
  graphs) (if any) or dependencies with other software?

# Reviewer 4

## Detailed Comments for Authors

- The paper presents a reliability analysis of the Titan
  SuperComputer. The authors perform time between failures analysis
  and statistical survival analysis on operational datasets that also
  are publicly made available. The main finding appears that GPU
  reliability is dependent on heat dissipation to an extent that
  strongly correlates with detailed nuances of the system cooling
  architecture.

- The paper presents a valuable analysis on a real dataset. It is
  definitely positive that datasets and methodologies are openly
  provided for further analysis. The main findings are mostly
  positional, and thus correlated with airflow. Are there more
  quantitative values that can can further reinforce this correlation?
  I.e., for differences in col 0-11 from col 12-24, while there is the
  speculation of different flows or pressure, the authors also report
  temperature.

- The last part of the analysis seems written quickly, and could be
  better streamlined. There also are a few typos. The paper seems to
  be missing a proper conclusion, which is embedded in the
  discussion. One question, given the facility involved in this
  retrospective, is if the analysis has influenced somehow the current
  and the upcoming (GPU based) systems.


## Comments for Revision

- Please improve the quality of the last part of the analysis
- Please provide where possible some more qualitative values related
  to issues with airflow/temperature

- Please see if it possible to provide a proper conclusion.
